name: Scrape and Monitor

on:
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:

# FIX 3: Give the runner write permissions
permissions:
  contents: write

jobs:
  monitor_pipeline:
    runs-on: ubuntu-latest
    env:
      # FIX 1: Set PYTHONPATH globally for the job
      PYTHONPATH: ${{ github.workspace }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          pip install ntscraper

      - name: Create Directory Structure
        # FIX 2: Ensure data paths exist for training
        run: mkdir -p data/raw data/processed data/new_scraped models

      - name: Run Scraper
        env:
          X_BEARER_TOKEN: ${{ secrets.X_BEARER_TOKEN }}
        run: python src/realtime_scraper.py

      - name: Monitor Performance
        id: monitoring
        run: python src/monitor.py
        continue-on-error: true

      - name: Trigger Retraining
        if: steps.monitoring.outcome == 'failure'
        run: |
          # If retraining needs the original data, ensure ingestion runs first
          python src/ingestion.py 
          python src/preprocessing.py
          python src/train.py
          
      - name: Commit and Push Updates
        if: always()
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          touch data/new_scraped/.gitkeep
          git add data/new_scraped/ models/
          git commit -m "Automated CT: Updated data and model artifacts" || echo "No changes to commit"
          git push